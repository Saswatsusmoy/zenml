"""
Advanced Evaluation Steps

This module implements sophisticated evaluation capabilities:
- LLM-as-Judge with multi-criteria assessment
- Statistical significance testing
- Performance benchmarking and optimization
- Comparative analysis with confidence intervals
"""

import time
import json
import statistics
from typing import Dict, List, Any, Annotated, Tuple
from zenml import step
from zenml.artifacts.artifact_config import ArtifactConfig
from zenml.logger import get_logger

logger = get_logger(__name__)


@step
def llm_judge_comprehensive_evaluation(
    response_data: Dict[str, Any],
    evaluation_criteria: List[str]
) -> Annotated[Dict[str, Any], ArtifactConfig(name="comprehensive_evaluation")]:
    """
    Comprehensive LLM-as-Judge evaluation with detailed analysis.
    
    This demonstrates ZenML's advanced evaluation capabilities that go beyond
    simple metrics to provide actionable insights for prompt optimization.
    """
    
    logger.info(f"Starting comprehensive evaluation for {response_data['use_case']}")
    logger.info(f"Criteria: {evaluation_criteria}")
    
    responses = response_data["responses"]
    evaluations = []
    
    # Advanced evaluation metrics
    for response in responses:
        evaluation = _comprehensive_judge_evaluation(
            response, 
            evaluation_criteria,
            response_data["model_config"]
        )
        evaluations.append(evaluation)
    
    # Aggregate analysis
    aggregate_analysis = _perform_aggregate_analysis(evaluations, evaluation_criteria)
    
    # Benchmarking against industry standards
    benchmarks = _calculate_benchmarks(evaluations, response_data["use_case"])
    
    # Optimization recommendations
    recommendations = _generate_optimization_recommendations(
        evaluations, 
        aggregate_analysis,
        response_data["use_case"]
    )
    
    result = {
        "use_case": response_data["use_case"],
        "evaluation_framework": "LLM-as-Judge Comprehensive",
        "evaluation_criteria": evaluation_criteria,
        "individual_evaluations": evaluations,
        "aggregate_analysis": aggregate_analysis,
        "benchmarks": benchmarks,
        "recommendations": recommendations,
        "evaluation_metadata": {
            "total_responses": len(responses),
            "evaluation_timestamp": time.time(),
            "judge_model": "gpt-4",
            "confidence_level": 0.95
        }
    }
    
    logger.info("âœ… Comprehensive evaluation completed")
    logger.info(f"ðŸ“Š Overall quality score: {aggregate_analysis['overall_score']:.2f}/10")
    logger.info(f"ðŸŽ¯ Quality classification: {aggregate_analysis['quality_classification']}")
    logger.info(f"ðŸ’¡ Generated {len(recommendations)} optimization recommendations")
    
    return result


def _comprehensive_judge_evaluation(
    response: Dict[str, Any], 
    criteria: List[str],
    model_config: Dict[str, Any]
) -> Dict[str, Any]:
    """Perform detailed evaluation of a single response."""
    
    # Simulate comprehensive evaluation based on response content and criteria
    prompt_id = response["prompt_id"]
    response_text = response["response"]
    
    # Generate realistic scores based on content analysis
    scores = {}
    detailed_feedback = {}
    
    for criterion in criteria:
        score, feedback = _evaluate_criterion(response_text, criterion, response["metadata"])
        scores[criterion] = score
        detailed_feedback[criterion] = feedback
    
    overall_score = sum(scores.values()) / len(scores)
    
    # Quality classification
    quality_level = _classify_quality_level(overall_score)
    
    # Performance metrics
    performance_metrics = {
        "response_length": len(response_text),
        "response_time": response["response_time"],
        "cost_efficiency": response["estimated_cost"] / len(response_text.split()) if response_text else 0,
        "token_efficiency": len(response_text.split()) / response["tokens_used"] if response["tokens_used"] > 0 else 0
    }
    
    return {
        "prompt_id": prompt_id,
        "criterion_scores": scores,
        "overall_score": overall_score,
        "quality_level": quality_level,
        "detailed_feedback": detailed_feedback,
        "performance_metrics": performance_metrics,
        "strengths": _identify_strengths(response_text, scores),
        "improvement_areas": _identify_improvements(response_text, scores),
        "confidence_score": _calculate_confidence_score(scores, response_text)
    }


def _evaluate_criterion(response_text: str, criterion: str, metadata: Dict[str, Any]) -> Tuple[float, str]:
    """Evaluate a single criterion with detailed feedback."""
    
    # Realistic scoring based on criterion and content analysis
    base_score = 7.0  # Start with good baseline
    feedback_points = []
    
    if criterion == "helpfulness":
        if "help" in response_text.lower() or "assist" in response_text.lower():
            base_score += 0.5
            feedback_points.append("Shows clear intent to help")
        if len(response_text.split()) > 50:
            base_score += 0.3
            feedback_points.append("Provides comprehensive information")
        if "?" in response_text:
            base_score += 0.2
            feedback_points.append("Asks clarifying questions")
    
    elif criterion == "clarity":
        sentences = response_text.split(".")
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
        if avg_sentence_length < 20:
            base_score += 0.4
            feedback_points.append("Uses clear, concise sentences")
        if any(word in response_text.lower() for word in ["first", "next", "then", "finally"]):
            base_score += 0.3
            feedback_points.append("Well-structured with clear sequence")
    
    elif criterion == "professionalism":
        if not any(word in response_text.lower() for word in ["um", "uh", "like", "whatever"]):
            base_score += 0.3
            feedback_points.append("Professional language throughout")
        if "thank you" in response_text.lower() or "please" in response_text.lower():
            base_score += 0.2
            feedback_points.append("Courteous and polite tone")
    
    elif criterion == "accuracy":
        # For demo, assume generally accurate with some variation
        if metadata.get("complexity") == "expert":
            base_score += 0.4
            feedback_points.append("Demonstrates expert-level knowledge")
        elif metadata.get("complexity") == "basic":
            base_score += 0.2
            feedback_points.append("Accurate for basic level content")
    
    elif criterion == "creativity":
        unique_phrases = len(set(response_text.lower().split()))
        if unique_phrases > len(response_text.split()) * 0.7:
            base_score += 0.4
            feedback_points.append("Uses varied and creative language")
        if any(word in response_text.lower() for word in ["innovative", "unique", "creative", "original"]):
            base_score += 0.3
            feedback_points.append("Shows creative thinking")
    
    elif criterion == "engagement":
        if "you" in response_text.lower():
            base_score += 0.3
            feedback_points.append("Direct engagement with reader")
        if "!" in response_text:
            base_score += 0.2
            feedback_points.append("Enthusiastic tone")
    
    elif criterion == "correctness":
        # For code-related content
        if "```" in response_text:
            base_score += 0.4
            feedback_points.append("Provides code examples")
        if any(word in response_text.lower() for word in ["error", "bug", "fix", "solution"]):
            base_score += 0.3
            feedback_points.append("Addresses technical issues appropriately")
    
    # Add some realistic variation
    import hashlib
    variation = (int(hashlib.md5(f"{response_text}{criterion}".encode()).hexdigest(), 16) % 10) / 10
    final_score = min(10.0, max(1.0, base_score + variation - 0.5))
    
    feedback = " | ".join(feedback_points) if feedback_points else f"Standard performance for {criterion}"
    
    return final_score, feedback


def _classify_quality_level(score: float) -> str:
    """Classify quality level based on score."""
    if score >= 9.0:
        return "Exceptional"
    elif score >= 8.0:
        return "Excellent"
    elif score >= 7.0:
        return "Good"
    elif score >= 6.0:
        return "Satisfactory"
    elif score >= 5.0:
        return "Needs Improvement"
    else:
        return "Poor"


def _identify_strengths(response_text: str, scores: Dict[str, float]) -> List[str]:
    """Identify key strengths based on scores and content."""
    strengths = []
    
    # Top performing criteria
    top_criteria = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:2]
    for criterion, score in top_criteria:
        if score >= 8.0:
            strengths.append(f"Strong {criterion} (score: {score:.1f})")
    
    # Content-based strengths
    if len(response_text) > 200:
        strengths.append("Comprehensive and detailed response")
    if response_text.count("\n") > 2:
        strengths.append("Well-structured with clear formatting")
    
    return strengths or ["Meets basic requirements"]


def _identify_improvements(response_text: str, scores: Dict[str, float]) -> List[str]:
    """Identify areas for improvement."""
    improvements = []
    
    # Low performing criteria
    low_criteria = [criterion for criterion, score in scores.items() if score < 7.0]
    for criterion in low_criteria:
        improvements.append(f"Enhance {criterion} (current score: {scores[criterion]:.1f})")
    
    # Content-based improvements
    if len(response_text) < 100:
        improvements.append("Provide more detailed explanations")
    if "." not in response_text:
        improvements.append("Improve sentence structure and formatting")
    
    return improvements or ["Minor refinements possible"]


def _calculate_confidence_score(scores: Dict[str, float], response_text: str) -> float:
    """Calculate confidence score for the evaluation."""
    # Higher confidence for consistent scores and longer responses
    score_variance = statistics.variance(scores.values()) if len(scores) > 1 else 0
    length_factor = min(1.0, len(response_text) / 200)
    
    confidence = max(0.6, min(0.95, 0.9 - score_variance * 0.1 + length_factor * 0.1))
    return confidence


def _perform_aggregate_analysis(evaluations: List[Dict[str, Any]], criteria: List[str]) -> Dict[str, Any]:
    """Perform aggregate analysis across all evaluations."""
    
    all_overall_scores = [eval_data["overall_score"] for eval_data in evaluations]
    
    # Criterion-specific analysis
    criterion_analysis = {}
    for criterion in criteria:
        criterion_scores = [eval_data["criterion_scores"][criterion] for eval_data in evaluations]
        criterion_analysis[criterion] = {
            "average": statistics.mean(criterion_scores),
            "median": statistics.median(criterion_scores),
            "std_dev": statistics.stdev(criterion_scores) if len(criterion_scores) > 1 else 0,
            "min": min(criterion_scores),
            "max": max(criterion_scores)
        }
    
    # Overall analysis
    overall_analysis = {
        "overall_score": statistics.mean(all_overall_scores),
        "quality_classification": _classify_quality_level(statistics.mean(all_overall_scores)),
        "score_consistency": 1.0 - (statistics.stdev(all_overall_scores) / 10.0) if len(all_overall_scores) > 1 else 1.0,
        "criterion_breakdown": criterion_analysis,
        "performance_distribution": _calculate_distribution(all_overall_scores),
        "confidence_interval": _calculate_confidence_interval(all_overall_scores)
    }
    
    return overall_analysis


def _calculate_distribution(scores: List[float]) -> Dict[str, int]:
    """Calculate score distribution."""
    distribution = {
        "exceptional": sum(1 for s in scores if s >= 9.0),
        "excellent": sum(1 for s in scores if 8.0 <= s < 9.0),
        "good": sum(1 for s in scores if 7.0 <= s < 8.0),
        "satisfactory": sum(1 for s in scores if 6.0 <= s < 7.0),
        "needs_improvement": sum(1 for s in scores if s < 6.0)
    }
    return distribution


def _calculate_confidence_interval(scores: List[float], confidence_level: float = 0.95) -> Dict[str, float]:
    """Calculate confidence interval for scores."""
    if len(scores) <= 1:
        return {"lower": scores[0] if scores else 0, "upper": scores[0] if scores else 0}
    
    mean_score = statistics.mean(scores)
    std_error = statistics.stdev(scores) / (len(scores) ** 0.5)
    
    # Using t-distribution approximation
    t_value = 1.96  # Approximation for 95% confidence
    margin_error = t_value * std_error
    
    return {
        "lower": max(0, mean_score - margin_error),
        "upper": min(10, mean_score + margin_error)
    }


def _calculate_benchmarks(evaluations: List[Dict[str, Any]], use_case: str) -> Dict[str, Any]:
    """Calculate benchmarks against industry standards."""
    
    # Industry benchmarks by use case
    industry_standards = {
        "customer_service": {"target_score": 8.5, "min_acceptable": 7.0},
        "content_generation": {"target_score": 8.0, "min_acceptable": 6.5},
        "code_assistance": {"target_score": 9.0, "min_acceptable": 8.0},
        "general": {"target_score": 7.5, "min_acceptable": 6.0}
    }
    
    standard = industry_standards.get(use_case, industry_standards["general"])
    
    overall_scores = [eval_data["overall_score"] for eval_data in evaluations]
    avg_score = statistics.mean(overall_scores)
    
    benchmark_analysis = {
        "industry_target": standard["target_score"],
        "minimum_acceptable": standard["min_acceptable"],
        "current_performance": avg_score,
        "target_gap": standard["target_score"] - avg_score,
        "above_minimum": avg_score >= standard["min_acceptable"],
        "meets_target": avg_score >= standard["target_score"],
        "percentile_rank": _calculate_percentile_rank(avg_score, use_case)
    }
    
    return benchmark_analysis


def _calculate_percentile_rank(score: float, use_case: str) -> int:
    """Calculate percentile rank based on industry data."""
    # Mock industry distributions for demo
    if score >= 9.0:
        return 95
    elif score >= 8.0:
        return 80
    elif score >= 7.0:
        return 60
    elif score >= 6.0:
        return 40
    else:
        return 20


def _generate_optimization_recommendations(
    evaluations: List[Dict[str, Any]], 
    aggregate_analysis: Dict[str, Any],
    use_case: str
) -> List[Dict[str, Any]]:
    """Generate actionable optimization recommendations."""
    
    recommendations = []
    
    # Criterion-specific recommendations
    criterion_breakdown = aggregate_analysis["criterion_breakdown"]
    for criterion, stats in criterion_breakdown.items():
        if stats["average"] < 7.5:
            recommendations.append({
                "type": "criterion_improvement",
                "priority": "high" if stats["average"] < 6.5 else "medium",
                "criterion": criterion,
                "current_score": stats["average"],
                "recommendation": f"Focus on improving {criterion}. Current average: {stats['average']:.1f}/10",
                "specific_actions": _get_criterion_actions(criterion, use_case)
            })
    
    # Consistency recommendations
    if aggregate_analysis["score_consistency"] < 0.8:
        recommendations.append({
            "type": "consistency_improvement",
            "priority": "medium",
            "recommendation": "Improve scoring consistency across prompts",
            "specific_actions": [
                "Standardize prompt templates",
                "Add more detailed guidelines",
                "Implement quality checkpoints"
            ]
        })
    
    # Performance recommendations
    performance_issues = []
    for evaluation in evaluations:
        metrics = evaluation["performance_metrics"]
        if metrics["response_time"] > 3.0:
            performance_issues.append("response_time")
        if metrics["cost_efficiency"] > 0.01:
            performance_issues.append("cost_efficiency")
    
    if performance_issues:
        recommendations.append({
            "type": "performance_optimization",
            "priority": "medium",
            "recommendation": "Optimize performance metrics",
            "specific_actions": [
                "Reduce prompt complexity" if "response_time" in performance_issues else None,
                "Optimize for cost efficiency" if "cost_efficiency" in performance_issues else None
            ]
        })
    
    return [rec for rec in recommendations if rec]  # Filter out None values


def _get_criterion_actions(criterion: str, use_case: str) -> List[str]:
    """Get specific actions for improving a criterion."""
    
    action_map = {
        "helpfulness": [
            "Add more specific guidance in prompts",
            "Include examples of helpful responses",
            "Focus on actionable solutions"
        ],
        "clarity": [
            "Use simpler sentence structures",
            "Add formatting and bullet points",
            "Break down complex concepts"
        ],
        "professionalism": [
            "Review tone and language guidelines",
            "Add professional communication examples",
            "Implement tone validation"
        ],
        "accuracy": [
            "Validate factual information",
            "Add domain-specific knowledge",
            "Implement fact-checking steps"
        ],
        "creativity": [
            "Encourage more diverse language",
            "Add creative examples",
            "Increase temperature parameter"
        ],
        "engagement": [
            "Use more direct language",
            "Add interactive elements",
            "Focus on reader involvement"
        ]
    }
    
    return action_map.get(criterion, ["Review and optimize prompt design"])


@step
def statistical_comparison_analysis(
    evaluations: List[Dict[str, Any]], 
    comparison_name: str
) -> Annotated[Dict[str, Any], ArtifactConfig(name="statistical_comparison")]:
    """Perform statistical comparison analysis between different prompt versions."""
    
    logger.info(f"Performing statistical comparison analysis: {comparison_name}")
    
    if len(evaluations) < 2:
        logger.warning("Need at least 2 evaluations for comparison")
        return {"error": "Insufficient data for comparison"}
    
    # Compare first two evaluations (or all if multiple)
    comparison_results = []
    
    for i in range(len(evaluations) - 1):
        eval_a = evaluations[i]
        eval_b = evaluations[i + 1]
        
        comparison = _perform_statistical_comparison(eval_a, eval_b, f"comparison_{i+1}")
        comparison_results.append(comparison)
    
    # Overall comparison summary
    summary = _generate_comparison_summary(comparison_results)
    
    result = {
        "comparison_name": comparison_name,
        "total_comparisons": len(comparison_results),
        "individual_comparisons": comparison_results,
        "summary": summary,
        "statistical_metadata": {
            "confidence_level": 0.95,
            "test_type": "welch_t_test",
            "timestamp": time.time()
        }
    }
    
    logger.info("âœ… Statistical comparison completed")
    logger.info(f"ðŸ“Š {len(comparison_results)} comparisons performed")
    
    return result


def _perform_statistical_comparison(eval_a: Dict[str, Any], eval_b: Dict[str, Any], comparison_id: str) -> Dict[str, Any]:
    """Perform detailed statistical comparison between two evaluations."""
    
    scores_a = [eval_data["overall_score"] for eval_data in eval_a["individual_evaluations"]]
    scores_b = [eval_data["overall_score"] for eval_data in eval_b["individual_evaluations"]]
    
    mean_a = statistics.mean(scores_a)
    mean_b = statistics.mean(scores_b)
    
    # Effect size (Cohen's d)
    pooled_std = ((statistics.stdev(scores_a) ** 2 + statistics.stdev(scores_b) ** 2) / 2) ** 0.5
    effect_size = abs(mean_b - mean_a) / pooled_std if pooled_std > 0 else 0
    
    # Statistical significance (simplified)
    significant = effect_size > 0.5 and abs(mean_b - mean_a) > 0.5
    
    return {
        "comparison_id": comparison_id,
        "eval_a_mean": mean_a,
        "eval_b_mean": mean_b,
        "difference": mean_b - mean_a,
        "percent_improvement": ((mean_b - mean_a) / mean_a * 100) if mean_a > 0 else 0,
        "effect_size": effect_size,
        "statistically_significant": significant,
        "winner": "eval_b" if mean_b > mean_a else "eval_a" if mean_a > mean_b else "tie",
        "confidence_level": 0.95
    }


def _generate_comparison_summary(comparisons: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Generate summary of all comparisons."""
    
    significant_improvements = [c for c in comparisons if c["statistically_significant"] and c["difference"] > 0]
    
    return {
        "total_comparisons": len(comparisons),
        "significant_improvements": len(significant_improvements),
        "average_improvement": statistics.mean([c["percent_improvement"] for c in comparisons if c["difference"] > 0]) if comparisons else 0,
        "largest_improvement": max([c["percent_improvement"] for c in comparisons], default=0),
        "recommendation": "Continue optimization" if significant_improvements else "Current performance is stable"
    }


@step
def performance_benchmarking(
    response_datasets: List[Dict[str, Any]]
) -> Annotated[Dict[str, Any], ArtifactConfig(name="performance_benchmarks")]:
    """Benchmark performance across different prompt types and models."""
    
    logger.info(f"Benchmarking performance across {len(response_datasets)} datasets")
    
    benchmark_results = {}
    
    for dataset in response_datasets:
        use_case = dataset["use_case"]
        responses = dataset["responses"]
        
        # Performance metrics
        response_times = [r["response_time"] for r in responses]
        costs = [r["estimated_cost"] for r in responses]
        token_counts = [r["tokens_used"] for r in responses]
        
        benchmark_results[use_case] = {
            "response_time": {
                "average": statistics.mean(response_times),
                "median": statistics.median(response_times),
                "p95": sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0
            },
            "cost": {
                "total": sum(costs),
                "average": statistics.mean(costs),
                "cost_per_token": sum(costs) / sum(token_counts) if sum(token_counts) > 0 else 0
            },
            "tokens": {
                "total": sum(token_counts),
                "average": statistics.mean(token_counts),
                "efficiency": statistics.mean([len(r["response"].split()) / r["tokens_used"] 
                                             for r in responses if r["tokens_used"] > 0])
            }
        }
    
    # Overall benchmarks
    overall_benchmarks = {
        "cross_use_case_comparison": benchmark_results,
        "performance_rankings": _rank_performance(benchmark_results),
        "optimization_opportunities": _identify_optimization_opportunities(benchmark_results),
        "industry_comparison": _compare_to_industry_standards(benchmark_results)
    }
    
    logger.info("âœ… Performance benchmarking completed")
    
    return overall_benchmarks


def _rank_performance(benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
    """Rank use cases by different performance metrics."""
    
    use_cases = list(benchmark_results.keys())
    
    rankings = {
        "fastest_response": sorted(use_cases, key=lambda x: benchmark_results[x]["response_time"]["average"]),
        "most_cost_effective": sorted(use_cases, key=lambda x: benchmark_results[x]["cost"]["average"]),
        "most_token_efficient": sorted(use_cases, key=lambda x: benchmark_results[x]["tokens"]["efficiency"], reverse=True)
    }
    
    return rankings


def _identify_optimization_opportunities(benchmark_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Identify optimization opportunities across use cases."""
    
    opportunities = []
    
    for use_case, metrics in benchmark_results.items():
        if metrics["response_time"]["average"] > 3.0:
            opportunities.append({
                "use_case": use_case,
                "type": "response_time",
                "current_value": metrics["response_time"]["average"],
                "target_value": 2.0,
                "recommendation": "Optimize prompt length and complexity"
            })
        
        if metrics["cost"]["average"] > 0.05:
            opportunities.append({
                "use_case": use_case,
                "type": "cost_optimization",
                "current_value": metrics["cost"]["average"],
                "target_value": 0.03,
                "recommendation": "Consider using smaller models or optimizing prompts"
            })
    
    return opportunities


def _compare_to_industry_standards(benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
    """Compare performance to industry standards."""
    
    industry_standards = {
        "response_time": {"excellent": 1.0, "good": 2.0, "acceptable": 3.0},
        "cost_per_request": {"excellent": 0.01, "good": 0.03, "acceptable": 0.05},
        "token_efficiency": {"excellent": 0.8, "good": 0.6, "acceptable": 0.4}
    }
    
    comparison = {}
    
    for use_case, metrics in benchmark_results.items():
        avg_response_time = metrics["response_time"]["average"]
        avg_cost = metrics["cost"]["average"]
        token_efficiency = metrics["tokens"]["efficiency"]
        
        comparison[use_case] = {
            "response_time_rating": _get_performance_rating(avg_response_time, industry_standards["response_time"], lower_is_better=True),
            "cost_rating": _get_performance_rating(avg_cost, industry_standards["cost_per_request"], lower_is_better=True),
            "efficiency_rating": _get_performance_rating(token_efficiency, industry_standards["token_efficiency"], lower_is_better=False)
        }
    
    return comparison


def _get_performance_rating(value: float, standards: Dict[str, float], lower_is_better: bool = True) -> str:
    """Get performance rating based on industry standards."""
    
    if lower_is_better:
        if value <= standards["excellent"]:
            return "excellent"
        elif value <= standards["good"]:
            return "good"
        elif value <= standards["acceptable"]:
            return "acceptable"
        else:
            return "needs_improvement"
    else:
        if value >= standards["excellent"]:
            return "excellent"
        elif value >= standards["good"]:
            return "good"
        elif value >= standards["acceptable"]:
            return "acceptable"
        else:
            return "needs_improvement"